{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SS5qUjwiKQhK"
   },
   "source": [
    "![ga4](https://www.google-analytics.com/collect?v=2&tid=G-6VDTYWLKX6&cid=1&en=page_view&sid=1&dl=statmike%2Fvertex-ai-mlops%2FApplied+GenAI&dt=Vertex+AI+GenAI+For+Document+Q%26A+-+MLB+Rules+For+Baseball.ipynb)\n",
    "\n",
    "# UmpireBot - MLB Rules For Baseball\n",
    "\n",
    "**What?**\n",
    "\n",
    "Ask questions of the rules for MLB and get answers with specific references to official rules.\n",
    "\n",
    "**Sources:**\n",
    "\n",
    "The official MLB rules are at [this link](https://www.mlb.com/official-information).  This is a 192 page PDF document.\n",
    "\n",
    "**Tools:**\n",
    "- Vertex AI LLM Embedding API\n",
    "  - `vertexai.preview.language_models.TextEmbeddingModel.from_pretrained('embedding-gecko@001')`\n",
    "- Vertex AI GenAI Language Model API\n",
    "  - `vertexai.preview.language_models.TextGenerationModel.from_pretrained('text@bison-001')`\n",
    "- Input file processing into documents/elements with GCP Document AI\n",
    "  - `google.cloud.documentai.DocumentProcessorServiceClient()`\n",
    "- Embedding search: with [ScaNN](https://github.com/google-research/google-research/tree/master/scann), [chromadb](https://github.com/chroma-core/chroma), [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview)\n",
    "  - this example uses `ScaNN` in the notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Google Cloud Vertex AI Generative AI Support**\n",
    "\n",
    "Vertex AI Generative AI gives access to Google's large genearative AI models and also enables you to test, tune, and deploy them for your applications. Get an overview [here](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview).\n",
    "\n",
    "---\n",
    "\n",
    "**How:**\n",
    "\n",
    "- Create documents from the source information.\n",
    "  - <u>Definition</u>: a document is a section of the rules\n",
    "- Create embeddings for each document\n",
    "- Ask a question\n",
    "  - Create an embedding of the question\n",
    "  - Use vector similarity to retrieve embeddings for related documents\n",
    "  - Retrieve the documents associated with returned embeddings\n",
    "  - Prepare a prompt to answer the question using the documents as context\n",
    "- Present the response as an answer with links to the related documents (sections of the document).\n",
    "\n",
    "**What is Unique?**\n",
    "\n",
    "An LLM is likely unexposed to a users private content.  This appoach constructs a summarization prompt for an LLM by first retriving context for the question from the users documents using embeddings.  This also allows the response to be accompanied by direct reference to the users documentation used in the prompt.\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- An LLM is likely trained on many sources that probably include lots of general knowledge, even information like what is used here.  It is also likely knowledgable of past outdated information which can be a benefit - or detriment - to accuracy.  This approach directly uses the version of the document that currently applies.\n",
    "\n",
    "**References**\n",
    "\n",
    "- [Vertex AI GenAI Studio](https://cloud.google.com/vertex-ai/docs/generative-ai/learn/overview)\n",
    "    - Vertex AI [Python Client](https://cloud.google.com/python/docs/reference/aiplatform/latest)\n",
    "- [Document AI](https://cloud.google.com/document-ai/docs/overview)\n",
    "    - Document AI [Parsers](https://cloud.google.com/document-ai/docs/processors-list)\n",
    "    - Document AI [Python Client](https://cloud.google.com/python/docs/reference/documentai/latest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R0Q3cgMcshvL"
   },
   "source": [
    "---\n",
    "## Overview\n",
    "\n",
    "<p><center>\n",
    "    <img alt=\"Overview Chart\" src=\"../architectures/notebooks/applied/genai/doc_qa.png\" width=\"55%\">\n",
    "</center><p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "od_UkDpvRmgD"
   },
   "source": [
    "---\n",
    "## Colab Setup\n",
    "\n",
    "To run this notebook in Colab click [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/statmike/vertex-ai-mlops/blob/main/Applied%20GenAI/Vertex%20AI%20GenAI%20For%20Document%20Q&A%20-%20MLB%20Rules%20For%20Baseball.ipynb) and run the cells in this section.  Otherwise, skip this section.\n",
    "\n",
    "This cell will authenticate to GCP (follow prompts in the popup)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 195,
     "status": "ok",
     "timestamp": 1683726184843,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8UO9FnqyKBlF"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = 'genai-test-project-jun4' # replace with project ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 68869,
     "status": "ok",
     "timestamp": 1683726253709,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "N98-KK7LRkjm",
    "outputId": "09ec5008-0def-4e1a-c349-c598ee752f78"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    !gcloud config set project {PROJECT_ID}\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROJECT_NUMBER_LIST = !gcloud projects list \\\n",
    "#--filter=\"$(gcloud config get-value project)\" \\\n",
    "#--format=\"value(PROJECT_NUMBER)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PROJECT_NUMBER = PROJECT_NUMBER_LIST[0]\n",
    "#print(PROJECT_NUMBER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build compute_engine_default_service_account [PROJECT_NUMBER]-compute@developer.gserviceaccount.com\n",
    "#compute_engine_default_service_account = PROJECT_NUMBER+'-compute@developer.gserviceaccount.com'\n",
    "#print(compute_engine_default_service_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n! gcloud projects add-iam-policy-binding genai-test-project-may28 --member=\"serviceAccount:388500005335-compute@developer.gserviceaccount.com\" --role=\"roles/aiplatform.user\"\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# role = roles/aiplatform.user\n",
    "'''\n",
    "! gcloud projects add-iam-policy-binding genai-test-project-may28 \\\n",
    "--member=\"serviceAccount:388500005335-compute@developer.gserviceaccount.com\" \\\n",
    "--role=\"roles/aiplatform.user\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdVRi9WBT7UQ"
   },
   "source": [
    "---\n",
    "## Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 107570,
     "status": "ok",
     "timestamp": 1683726361275,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "KX-2AeZvUANA",
    "outputId": "6c9ed89c-0266-4aa7-f3d5-ce49ed3aed21",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.10/site-packages (1.25.0)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.34.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.22.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5 in /home/jupyter/.local/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.19.6)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (2.8.0)\n",
      "Requirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (3.10.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.10.0)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.59.0)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.17.3)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2.28.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.3)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.3.2)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.5.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.3.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.16.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (2022.12.7)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (0.5.0)\n"
     ]
    }
   ],
   "source": [
    "# Vertex AI GenAI Studio SDK - Update the aiplatform SDK\n",
    "#!pip install google.cloud.aiplatform -U -q --user\n",
    "! pip3 install --upgrade google-cloud-aiplatform\n",
    "\n",
    "# for working with embeddings locally\n",
    "!pip3 install scann -q --user\n",
    "\n",
    "# For GCP DocAI and presentation of results\n",
    "!pip3 install google-cloud-documentai -q --user\n",
    "!pip3 install PyPDF2 -U -q --user\n",
    "!pip3 install Pillow -U -q --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 4026,
     "status": "ok",
     "timestamp": 1683726365283,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "mvpgmn_s4BRl"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    from google.colab import auth\n",
    "    auth.authenticate_user()\n",
    "    # Enable Document AI For This Project\n",
    "    !gcloud services enable documentai.googleapis.com\n",
    "    # Enable Vertex AI For This Project\n",
    "    !gcloud services enable aiplatform.googleapis.com\n",
    "except Exception:\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `aiplatform` version needs to be 1.25.0 or higher for the LLM functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.25.0'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "aiplatform.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1g3HIKRmGUmQ"
   },
   "source": [
    "**RESTART RUNTIME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "appt8-yVRtJ1"
   },
   "source": [
    "---\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63mx2EozRxFP"
   },
   "source": [
    "Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2124,
     "status": "ok",
     "timestamp": 1683726390544,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "xzcoXjM5Rky5",
    "outputId": "b3bdcbc1-70d5-472e-aea2-42c74a42efde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genai-test-project-jun4'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project = !gcloud config get-value project\n",
    "PROJECT_ID = project[0]\n",
    "PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1683726390712,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "IxWrFtqYMfku"
   },
   "outputs": [],
   "source": [
    "REGION = 'us-central1'\n",
    "EXPERIMENT = 'mlb-rules'\n",
    "SERIES = 'applied-genai'\n",
    "\n",
    "# save results in: GCS, BQ, ALL\n",
    "SAVE_IN = 'GCS'\n",
    "# retrieve results from: GCS, BQ.  If not present then it will run parsing and embedding.\n",
    "RETRIEVE_FROM = 'GCS'\n",
    "\n",
    "# make this the gcs bucket for storing files\n",
    "GCS_BUCKET = PROJECT_ID \n",
    "\n",
    "# make this the BQ Project / Dataset / Table prefix to store results\n",
    "BQ_PROJECT = PROJECT_ID\n",
    "BQ_DATASET = SERIES.replace('-', '_')\n",
    "BQ_TABLE = EXPERIMENT\n",
    "\n",
    "# location for the source document (PDF): can be http or gs://\n",
    "source_document = 'https://img.mlbstatic.com/mlb-images/image/upload/mlb/wqn5ah4c3qtivwx3jatm.pdf'\n",
    "#source_document = f'gs://{GCS_BUCKET}/{SERIES}/{EXPERIMENT}/wqn5ah4c3qtivwx3jatm.pdf'\n",
    "\n",
    "# first question to ask\n",
    "question = \"What are the rules for baseball?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuajVwCiO6Yg"
   },
   "source": [
    "Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "executionInfo": {
     "elapsed": 17761,
     "status": "ok",
     "timestamp": 1683726409304,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "LVC7zzSLRk2C"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'PyPDF2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfutures\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'PyPDF2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import concurrent.futures\n",
    "import time\n",
    "\n",
    "import PyPDF2\n",
    "import IPython\n",
    "import PIL\n",
    "import PIL.ImageFont, PIL.Image, PIL.ImageDraw\n",
    "import shapely\n",
    "\n",
    "import scann\n",
    "import numpy as np\n",
    "\n",
    "import vertexai.preview.language_models\n",
    "from google.cloud import aiplatform\n",
    "from google.cloud import documentai\n",
    "from google.cloud.documentai_v1 import Document\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "from vertexai.preview.language_models import TextEmbeddingModel\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EyAVFG9TO9H-"
   },
   "source": [
    "Clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1683726409306,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "L0RPE13LOZce"
   },
   "outputs": [],
   "source": [
    "# vertex ai clients\n",
    "vertexai.init(project = PROJECT_ID, location = REGION)\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)\n",
    "\n",
    "# document AI client\n",
    "LOCATION = REGION.split('-')[0]\n",
    "docai_client = documentai.DocumentProcessorServiceClient(\n",
    "    client_options = dict(api_endpoint = f\"{LOCATION}-documentai.googleapis.com\")\n",
    ")\n",
    "\n",
    "# bigquery client\n",
    "bq = bigquery.Client(project = PROJECT_ID)\n",
    "\n",
    "# gcs client: assumes bucket already exists\n",
    "gcs = storage.Client(project = PROJECT_ID)\n",
    "bucket = gcs.bucket(GCS_BUCKET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PF_zXB00TMao"
   },
   "source": [
    "---\n",
    "## Vertex LLM Setup\n",
    "\n",
    "- TextEmbeddingModel [Guide](https://cloud.google.com/vertex-ai/docs/generative-ai/embeddings/get-text-embeddings)\n",
    "    - TextEmbeddingModel [API](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.preview.language_models.TextEmbeddingModel)\n",
    "- TextGenerationModel [Guide](https://cloud.google.com/vertex-ai/docs/generative-ai/text/test-text-prompts)\n",
    "    - TextGenerationModel [API](https://cloud.google.com/python/docs/reference/aiplatform/latest/vertexai.preview.language_models.TextGenerationModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1683726409314,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "dn2FH-pETUPf"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# create links to model: embedding api and text generation\n",
    "\n",
    "\n",
    "#embedding_model = vertexai.preview.language_models.TextEmbeddingModel.from_pretrained('textembedding-gecko@001')\n",
    "#textgen_model = vertexai.preview.language_models.TextGenerationModel.from_pretrained('text-bison@001')\n",
    "embedding_model = TextEmbeddingModel.from_pretrained(\"textembedding-gecko@001\")\n",
    "textgen_model = TextGenerationModel.from_pretrained(\"text-bison@001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UvqBtnRbQH0v"
   },
   "source": [
    "Test embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embeddings = embedding_model.get_embeddings([\"What is life?\"])\n",
    "for embedding in embeddings:\n",
    "    vector = embedding.values\n",
    "    print(f'Length of Embedding Vector: {len(vector)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "embedding_model.get_embeddings([question])[0].values[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qEITqYF1QNkP"
   },
   "source": [
    "Test test generation (llm) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1758,
     "status": "ok",
     "timestamp": 1683726411751,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "ippLc2_eYeQH",
    "outputId": "c09300ed-9e44-40a7-f3d0-f4a95515021c"
   },
   "outputs": [],
   "source": [
    "textgen_model.predict(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ydo-up2TMwo1"
   },
   "source": [
    "---\n",
    "## Get/Create Document AI Processors\n",
    "\n",
    "Using the [General Form Processor](https://cloud.google.com/document-ai/docs/processors-list#general_processors) and [Document Splitter](https://cloud.google.com/document-ai/docs/processors-list#processor_doc-splitter).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 451,
     "status": "ok",
     "timestamp": 1683726412200,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "e4qZg7G-GiIm",
    "outputId": "4a6a7063-e11c-44c7-bfde-08b297f31cc3"
   },
   "outputs": [],
   "source": [
    "PARSER_DISPLAY_NAME = 'my_general_processor'\n",
    "PARSER_TYPE = 'FORM_PARSER_PROCESSOR'\n",
    "PARSER_VERSION = 'pretrained-form-parser-v2.0-2022-11-10'\n",
    "\n",
    "for p in docai_client.list_processors(parent = f'projects/{PROJECT_ID}/locations/{LOCATION}'):\n",
    "  if p.display_name == PARSER_DISPLAY_NAME:\n",
    "    parser = p\n",
    "    print('Retrieved Existing Parser')\n",
    "if 'parser' not in locals():\n",
    "  parser = docai_client.create_processor(\n",
    "      parent = f'projects/{PROJECT_ID}/locations/{LOCATION}',\n",
    "      processor = dict(display_name = PARSER_DISPLAY_NAME, type_ = PARSER_TYPE, default_processor_version = PARSER_VERSION)\n",
    "  )\n",
    "  print('Created New Parser')\n",
    "\n",
    "parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T-2fW-ybMwo3"
   },
   "source": [
    "---\n",
    "## Get The Document\n",
    "\n",
    "Get the source PDF from GCS or a URL and store as a list of pages: `pdfs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Location of File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if source_document.startswith('http'):\n",
    "    document_location = 'URL'\n",
    "    print('Use requests to get online document')\n",
    "elif source_document.startswith('gs'):\n",
    "    document_location = 'GCS'\n",
    "    print('Use GCS to get document in GCS')\n",
    "else:\n",
    "    document_location = 'UNKNOWN'\n",
    "    print(f'The source_document variable points to a document in an unknown location type (not gs:// or http): {source_document}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8PsjZxvTr14"
   },
   "source": [
    "Copy the PDF to memory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 239,
     "status": "ok",
     "timestamp": 1683726412436,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "MN9SYNb2TsEo"
   },
   "outputs": [],
   "source": [
    "if document_location == 'URL':\n",
    "    response = requests.get(source_document).content\n",
    "elif document_location == 'GCS':\n",
    "    blob = bucket.blob(source_document.split(f'gs://{GCS_BUCKET}/')[1])\n",
    "    response = blob.download_as_bytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1683726412437,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "zR4-vfUr3Rtj",
    "outputId": "a991a277-68b9-4646-a1ee-df9e212c94a3"
   },
   "outputs": [],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3sEH4iI0Mn_"
   },
   "source": [
    "Split the PDF into a document per page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1683726412438,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "so7ejmvf1U9_"
   },
   "outputs": [],
   "source": [
    "pdf = PyPDF2.PdfReader(io.BytesIO(response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 395,
     "status": "ok",
     "timestamp": 1683726412829,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "iyb-77cz1VCW",
    "outputId": "eec898eb-1e6f-4f51-d9fe-2288002f176d"
   },
   "outputs": [],
   "source": [
    "len(pdf.pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2288,
     "status": "ok",
     "timestamp": 1683726415114,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "kmvIGX4Z3RhF"
   },
   "outputs": [],
   "source": [
    "pdfs = []\n",
    "for page_num, page in enumerate(pdf.pages, 1):\n",
    "  writer = PyPDF2.PdfWriter()\n",
    "  writer.add_page(page)\n",
    "  with io.BytesIO() as bytes_stream:\n",
    "    pdfs.append(writer.write(bytes_stream)[1].getbuffer().tobytes())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ilogOjNqnkZP"
   },
   "source": [
    "---\n",
    "## Retrieve Files From Previous Run on GCS Or BigQuery\n",
    "\n",
    "This uses the input parameter set above: `RETRIEVE_FROM`.  If it is set to `BQ` or `GCS` then it will check the source for an available prior run and retrieve it if it exists.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to check for existance of BigQuery Table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bq_table_check(table):\n",
    "    from google.cloud.exceptions import NotFound\n",
    "    try:\n",
    "        bq.get_table(table)\n",
    "        return True\n",
    "    except NotFound:\n",
    "        return False\n",
    "    \n",
    "bq_table_check(f'{BQ_DATASET}.{BQ_TABLE}_documents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RETRIEVE_FROM == 'GCS' and len(list(bucket.list_blobs(prefix = f'{SERIES}/{EXPERIMENT}/results.json'))) > 0:\n",
    "    print('Copying previous run from GCS')\n",
    "    \n",
    "    # load results: the raw data from docai parsing\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/results.json')\n",
    "    results = [json.loads(line) for line in blob.download_as_text().splitlines()]\n",
    "    \n",
    "    # load documents: the prepared results from the docai parsing results\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/documents.json')\n",
    "    documents = [json.loads(line) for line in blob.download_as_text().splitlines()]\n",
    "    \n",
    "    # load page images for presentation: stored in the results\n",
    "    page_images = []\n",
    "    for r, result in enumerate(results):\n",
    "        document_image = PIL.Image.open(\n",
    "            io.BytesIO(\n",
    "                 # stored as string, use .encode() to convert to bytes, use base64.decodebytes to decode\n",
    "                base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8'))\n",
    "            )\n",
    "        )\n",
    "        page_images.append(document_image)    \n",
    "    \n",
    "    # Set Indicator to prevent redoing the parsing later in this notebook\n",
    "    PRIOR_PARSE = True\n",
    "elif RETRIEVE_FROM == 'BQ' and bq_table_check(f'{BQ_DATASET}.{BQ_TABLE}_documents'):\n",
    "    print('Copying previous run from BigQuery')\n",
    "    \n",
    "    # load results: the raw data from docai parsing\n",
    "    results = bq.query(f'SELECT * FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}_results` ORDER BY metadata.row').to_dataframe().to_dict('records')\n",
    "    \n",
    "    # load documents: the perpared results from the docai parsing results\n",
    "    documents = bq.query(f'SELECT * FROM `{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}_documents` ORDER BY metadata.row').to_dataframe().to_dict('records')\n",
    "    \n",
    "    # load page images for presentation: stored in the results\n",
    "    page_images = []\n",
    "    for r, result in enumerate(results):\n",
    "        document_image = PIL.Image.open(\n",
    "            io.BytesIO(\n",
    "                 # stored as base64 encoded bytes in BQ and retrieved in that format\n",
    "                result['pages'][0]['image']['content']\n",
    "            )\n",
    "        )\n",
    "        page_images.append(document_image) \n",
    "    \n",
    "    # Set Indicator to prevent redoing the parsing later in this notebook\n",
    "    PRIOR_PARSE = True\n",
    "else:\n",
    "    print('No previous run available to copy over')\n",
    "    PRIOR_PARSE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ph2FjuLWnRgD"
   },
   "source": [
    "---\n",
    "## Parse Documents\n",
    "\n",
    "Results of:\n",
    "- [google.cloud.documentai.DocumentProcessorServiceClient().process_document()](https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.services.document_processor_service.DocumentProcessorServiceClient#google_cloud_documentai_v1_services_document_processor_service_DocumentProcessorServiceClient_process_document)\n",
    "  - are in the format of\n",
    "    - [google.cloud.documentai_v1.types.ProcessResponse()](https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.types.ProcessResponse)\n",
    "      - which contains `.document` in the format of:\n",
    "        - [google.cloud.documentai_v1.types.Document](https://cloud.google.com/python/docs/reference/documentai/latest/google.cloud.documentai_v1.types.Document)\n",
    "\n",
    "Converting the Document to:\n",
    "- JSON with .to_json()\n",
    "- dictionary with .to_dict()\n",
    "\n",
    "**Document AI Notes:**\n",
    "- In this application we are using online processing.  This has a limit of 15 pages per document.  Switch to batch increases this to 100 pages for the Form Parser (General).\n",
    "- Online processing has a default qouta of 120 request per minute per project. The code below implement waiting time to avoid this limit.\n",
    "- [Reference](https://cloud.google.com/document-ai/quotas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683726425375,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "QH0zn3yJ3LdG"
   },
   "outputs": [],
   "source": [
    "rate_limit_minute = 120\n",
    "adjust_rate_limit = rate_limit_minute / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683726425376,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "r_li_wryamXT"
   },
   "outputs": [],
   "source": [
    "def docai_runner(p, start, raw_document):\n",
    "  sleep_time = (p * (60/adjust_rate_limit)) - (time.time() - start)\n",
    "  if sleep_time > 0: time.sleep(sleep_time)\n",
    "\n",
    "  return (p, docai_client.process_document(request = dict(raw_document = raw_document, name = parser.name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9102,
     "status": "ok",
     "timestamp": 1683726434473,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "eMSPlA4joZZp",
    "outputId": "8ae2645a-b7c8-4f94-b70a-92fc2ae6734a"
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('Using Prior Results')\n",
    "else:\n",
    "    print('No Prior Results, Parsing with Document AI')\n",
    "    print(f\"The Expected runtime for the parsing is {(len(pdfs)/adjust_rate_limit):.2f} minutes\")\n",
    "    results = [None] * len(pdfs)\n",
    "    start = time.time()\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers = len(pdfs)) as executor:\n",
    "        futures = [\n",
    "            executor.submit(\n",
    "                docai_runner,\n",
    "                p, start,\n",
    "                documentai.RawDocument(content = pdf, mime_type = 'application/pdf')\n",
    "            ) for p, pdf in enumerate(pdfs)\n",
    "        ]\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "          #result = futures[future]\n",
    "          results[future.result()[0]] = (Document.to_dict(future.result()[1].document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1683726434474,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "8HdMuoK8isWD",
    "outputId": "84109237-d9b7-4aa9-ece3-2b64b06cc2f5"
   },
   "outputs": [],
   "source": [
    "len(pdfs), len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3677,
     "status": "ok",
     "timestamp": 1683726438145,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "qZERyWTBl3zj",
    "outputId": "8401227c-0621-4d5a-a34b-6f9b53ae1a5f"
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "  print('Using Prior Documents Preparation')\n",
    "else:\n",
    "  documents = []\n",
    "  page_images = []\n",
    "  for r, result in enumerate(results):\n",
    "    \n",
    "    # add order to results:\n",
    "    results[r]['metadata'] = dict(row = r)\n",
    "    \n",
    "    document_image = PIL.Image.open(\n",
    "        io.BytesIO(\n",
    "            base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8'))\n",
    "            )\n",
    "        )\n",
    "    page_images.append(document_image)\n",
    "\n",
    "    tables = []\n",
    "    for t, table in enumerate(result['pages'][0]['tables']):\n",
    "      table_txt = ''\n",
    "      if 'text_anchor' in table['layout'].keys():\n",
    "        for s, segment in enumerate(table['layout']['text_anchor']['text_segments']):\n",
    "          if t == 0 and s == 0: start = 0\n",
    "          else: start = int(segment['start_index'])\n",
    "          end = int(segment['end_index'])\n",
    "          table_txt += result['text'][start:end+t]\n",
    "\n",
    "      vertices = []\n",
    "      for vertex in table['layout']['bounding_poly']['normalized_vertices']:\n",
    "        vertices.append(dict(x = vertex['x'] * document_image.size[0], y = vertex['y'] * document_image.size[1]))\n",
    "      tables.append(shapely.geometry.Polygon([(v['x'], v['y']) for v in vertices]))\n",
    "\n",
    "      documents.append(\n",
    "          dict(\n",
    "              page_content = table_txt,\n",
    "              metadata = dict(\n",
    "                  page = r+1,\n",
    "                  table = t+1,\n",
    "                  row = len(documents),\n",
    "                  filename = source_document.split('/')[-1],\n",
    "                  source_document = source_document\n",
    "              ),\n",
    "              extras = dict(\n",
    "                  #image = base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8')),\n",
    "                  vertices = vertices\n",
    "              )\n",
    "          )\n",
    "      )\n",
    "\n",
    "    for p, paragraph in enumerate(result['pages'][0]['paragraphs']):\n",
    "\n",
    "      paragraph_txt = ''\n",
    "      for s, segment in enumerate(paragraph['layout']['text_anchor']['text_segments']):\n",
    "        if p == 0 and s == 0: start = 0\n",
    "        else: start = int(segment['start_index'])\n",
    "        end = int(segment['end_index'])\n",
    "        paragraph_txt += result['text'][start:end+1]\n",
    "\n",
    "      vertices = []\n",
    "      for vertex in paragraph['layout']['bounding_poly']['normalized_vertices']:\n",
    "        vertices.append(dict(x = vertex['x'] * document_image.size[0], y = vertex['y'] * document_image.size[1]))\n",
    "\n",
    "      # only use paragraph that are not within table boundaries\n",
    "      use_paragraph = True\n",
    "      for t_shape in tables:\n",
    "        p_shape = shapely.geometry.Polygon([(v['x'], v['y']) for v in vertices])\n",
    "        if p_shape.intersects(t_shape): use_paragraph = False\n",
    "\n",
    "      if use_paragraph:\n",
    "        documents.append(\n",
    "            dict(\n",
    "                page_content = paragraph_txt,\n",
    "                metadata = dict(\n",
    "                    page = r+1,\n",
    "                    paragraph = p+1,\n",
    "                    row = len(documents),\n",
    "                    filename = source_document.split('/')[-1],\n",
    "                    source_document = source_document\n",
    "                ),\n",
    "                extras = dict(\n",
    "                    #image = base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8')),\n",
    "                    vertices = vertices\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1683726438146,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "4yRCr36-M3h2",
    "outputId": "1b11a575-6faa-4c1a-ab88-05e51e8016c6"
   },
   "outputs": [],
   "source": [
    "print(documents[300]['page_content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1683726438147,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "6g6f31EwXX4d",
    "outputId": "1530f335-6f19-4dc0-fc35-d8f5362e4ca6"
   },
   "outputs": [],
   "source": [
    "print(documents[300]['metadata'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6PUA8dwSKtzl"
   },
   "source": [
    "---\n",
    "## Get Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1683726438486,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "ChO9K7ngfiNV"
   },
   "outputs": [],
   "source": [
    "rate_limit_minute = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1683726438487,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "EIfjVw_M3haf",
    "outputId": "90209f4d-596d-40a3-b98f-131160c8efbf"
   },
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "  print('Embeddings created on previous run.')\n",
    "else:\n",
    "  print(f\"The expected run time for embeddings is {(len(documents)/rate_limit_minute):.2f} minutes\")\n",
    "  start = time.time()\n",
    "  for d, document in enumerate(documents):\n",
    "    if d % rate_limit_minute == 0:\n",
    "      time.sleep(((time.time() - start) % 60) + 10)\n",
    "      start = time.time()\n",
    "    text = document['page_content']\n",
    "    if text:\n",
    "      embed = embedding_model.get_embeddings([text])[0].values\n",
    "    else:\n",
    "      embed = []\n",
    "    documents[d]['embedding'] = embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "74inkdrbXXfo"
   },
   "source": [
    "## Create Embeddings Database\n",
    "\n",
    "\n",
    "Use [ScaNN](https://github.com/google-research/google-research/tree/master/scann) to build a local vector search capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1683726438487,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "aobRdNMsFIqJ",
    "outputId": "6943e91b-973c-4e7b-d4cd-884ae36f2281"
   },
   "outputs": [],
   "source": [
    "index = np.empty((len(documents), len(documents[0]['embedding'])))\n",
    "index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(results) == list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1683726438488,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "ahxQEoRcFIaq"
   },
   "outputs": [],
   "source": [
    "if type(documents[0]['embedding']) == list:\n",
    "    for i in range(index.shape[0]):\n",
    "        if documents[i]['page_content']:\n",
    "            index[i] = documents[i]['embedding']        \n",
    "elif type(documents[0]['embedding']) == np.ndarray: # retrieved from BigQuery\n",
    "    for i in range(index.shape[0]):\n",
    "        if documents[i]['page_content']:\n",
    "            index[i] = documents[i]['embedding'].tolist()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_index = index / np.linalg.norm(index, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4833,
     "status": "ok",
     "timestamp": 1683726443309,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "Tsab_xblmIdx"
   },
   "outputs": [],
   "source": [
    "# configure ScaNN as a tree - asymmetric hash hybrid with reordering\n",
    "# anisotropic quantization as described in the paper; see README\n",
    "\n",
    "# use scann.scann_ops.build() to instead create a TensorFlow-compatible searcher\n",
    "builder = scann.scann_ops_pybind.builder(\n",
    "    normalized_index, # index\n",
    "    10, # num_neighbors\n",
    "    \"dot_product\" # distance_measure\n",
    "    )\n",
    "\n",
    "searcher = builder.tree(\n",
    "    num_leaves=index.shape[0], #num_leaves\n",
    "    num_leaves_to_search=index.shape[0], #num_leaves_to_search\n",
    "    training_sample_size=index.shape[0]\n",
    "    ).score_ah(\n",
    "      2,\n",
    "      anisotropic_quantization_threshold=0.2\n",
    "      ).reorder(\n",
    "          index.shape[0]\n",
    "          ).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1683726443310,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "dboNN2pNmVmY"
   },
   "outputs": [],
   "source": [
    "def search_index(query, k):\n",
    "    query = embedding_model.get_embeddings([query])[0].values\n",
    "    neighbors, distances = searcher.search(query, final_num_neighbors=k)\n",
    "    return list(zip(neighbors, distances))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 454,
     "status": "ok",
     "timestamp": 1683726443759,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "Td-73nAOmVYi",
    "outputId": "4d9f884b-9b0a-40d3-9fb0-5f7ebcd7efb6"
   },
   "outputs": [],
   "source": [
    "search_index(question, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "udUMCH-ponRL"
   },
   "source": [
    "---\n",
    "## Save For Future Runs: GCS, BigQuery\n",
    "\n",
    "Use the values of the input parameter `SAVE_IN` to optionally write both `results` and `documents` to `BQ`, `GCS` or `ALL` (both).\n",
    "\n",
    "It can take awhile to run the parsing job above so save results for future runs of this notebook.  Also, this prevents recurring cost of running the Document AI parsing of the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PRIOR_PARSE:\n",
    "    print('This run loaded results from a prior run.  Not overwriting.')\n",
    "else:\n",
    "    if SAVE_IN in ['GCS', 'ALL']:\n",
    "        print('Writing contents of results and documents to GCS for future use.')\n",
    "\n",
    "        # save results: json lines\n",
    "        blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/results.json')\n",
    "        blob.upload_from_string('\\n'.join([json.dumps(result) for result in results]), content_type = 'application/json')\n",
    "\n",
    "        # save documents: json lines\n",
    "        blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/documents.json')\n",
    "        blob.upload_from_string('\\n'.join([json.dumps(document) for document in documents]), content_type = 'application/json')\n",
    "    if SAVE_IN in ['BQ', 'ALL']:\n",
    "        print('Writing contents of results and documents to BigQuery for future use.')\n",
    "\n",
    "        # create/link to dataset\n",
    "        ds = bigquery.DatasetReference(BQ_PROJECT, BQ_DATASET)\n",
    "        ds.location = REGION\n",
    "        ds.labels = {'series': f'{SERIES}', 'experiment': f'{EXPERIMENT}'}\n",
    "        ds = bq.create_dataset(dataset = ds, exists_ok = True)    \n",
    "\n",
    "        # make load job configuration\n",
    "        job_config = bigquery.LoadJobConfig(\n",
    "            source_format = bigquery.SourceFormat.NEWLINE_DELIMITED_JSON,\n",
    "            write_disposition = bigquery.WriteDisposition.WRITE_TRUNCATE, #.WRITE_APPEND, #.WRITE_TRUNCATE,\n",
    "            create_disposition = bigquery.CreateDisposition.CREATE_IF_NEEDED,\n",
    "            autodetect = True\n",
    "        )    \n",
    "\n",
    "        # save results\n",
    "        load_job = bq.load_table_from_json(\n",
    "            json_rows = results,\n",
    "            destination = ds.table(BQ_TABLE + '_results'),\n",
    "            job_config = job_config\n",
    "        )\n",
    "        load_job.result()   \n",
    "\n",
    "        # save documents\n",
    "        load_job = bq.load_table_from_json(\n",
    "            json_rows = documents,\n",
    "            destination = ds.table(BQ_TABLE + '_documents'),\n",
    "            job_config = job_config\n",
    "        )\n",
    "        load_job.result()       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on Schema Changes**\n",
    "\n",
    "When writing `results` to BigQuery the value of `results[*]['pages'][0]['image']['content']` is automatically converted to base64 encoding and is in `bytes` format.  When retrieve back to a Python variable this does not convert back to `string`.  This would results in needing to modify one line of the import parsing:\n",
    "\n",
    "```Python\n",
    "#FROM\n",
    "base64.decodebytes(result['pages'][0]['image']['content'].encode('utf-8'))\n",
    "#TO\n",
    "result['pages'][0]['image']['content']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Embeddings For Vertex AI Matching Engine\n",
    "\n",
    "The notebook [Vertex AI Matching Engine For Document Q&A](./Vertex%20AI%20Matching%20Engine%20For%20Document%20Q&A.ipynb) shows how to host the embeddings on [Vertex AI Matching Engine](https://cloud.google.com/vertex-ai/docs/matching-engine/overview) for stateful, low-latency vector searching.  The code below saves the embeddings in JSON, one of the [supported input formats](https://cloud.google.com/vertex-ai/docs/matching-engine/match-eng-setup/format-structure#data-file-formats) for Vertex AI Matching Engine index creation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_IN in ['GCS', 'ALL']:\n",
    "    print('Writing embeddings to GCS for Use in Vertex AI Matching Engine.')\n",
    "    # save results: json lines\n",
    "    blob = bucket.blob(f'{SERIES}/{EXPERIMENT}/vertex_matching/embeddings.json')\n",
    "    blob.upload_from_string('\\n'.join([json.dumps(dict(id = document['metadata']['row'], embedding = document['embedding'])) for document in documents]), content_type = 'application/json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eM__fmk2p6tp"
   },
   "source": [
    "---\n",
    "## Q&A With DocumentBot\n",
    "\n",
    "Make a function that receives the users questions and:\n",
    "- finds and retrieves relative sections of the rules\n",
    "- prepares a prompt for Vertex AI Generative AI that includes the question and the context = sections of document\n",
    "- Retrieves the response (answer) from Vertex AI Generative AI\n",
    "- Retrieves the closest match section of the rules to the response/answer.\n",
    "- Checks to see if the section closest to the response/answer was included in the sections of the rules provided in the prompt.\n",
    "- Prepares and presents all the information back to the user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_bot(question):\n",
    "\n",
    "    # Get the score for the closest match\n",
    "    score = search_index(question, k = 1)[0][1]\n",
    "    \n",
    "    # retrieve related documents - the nubmer is based on the distance score from the closest match\n",
    "    relevant_documentation = search_index(question, k = 1 + 2*int(10*(1-score)))\n",
    "\n",
    "    # prepare context for prompt\n",
    "    context = \"\\n\".join([f'Context {c+1}:\\n' + documents[doc[0]]['page_content'] for c, doc in enumerate(relevant_documentation)])\n",
    "    \n",
    "    # construct the prompt\n",
    "    prompt = f\"\"\"\n",
    "        Give a detailed answer to the question using information from the provided contexts.\n",
    "\n",
    "        {context}\n",
    "\n",
    "        Question:\n",
    "        {question}\n",
    "\n",
    "        Answer and Explanation:\n",
    "    \"\"\"\n",
    "\n",
    "    #print(prompt)\n",
    "\n",
    "    # retrieve response\n",
    "    response = textgen_model.predict(prompt)\n",
    "    \n",
    "    # get closest document to the response:\n",
    "    likely_source = search_index(response, k=1)[0]\n",
    "    \n",
    "    # declare likely source: if closest document to response was in context pick it, otherwise pick first context match\n",
    "    if likely_source[0] in [rd[0] for rd in relevant_documentation]:\n",
    "        likely = True\n",
    "        likely_document = documents[likely_source[0]]\n",
    "    else:\n",
    "        likely = False\n",
    "        likely_document = documents[relevant_documentation[0][0]]\n",
    "\n",
    "\n",
    "    sources = \"\\n\".join(f\"* {documents[doc[0]]['metadata']['source_document']}#page={documents[doc[0]]['metadata']['page']}\\n\\t* Document: {documents[doc[0]]['metadata']['filename']}, page: {documents[doc[0]]['metadata']['page']}, relevance to question: {doc[1]:.2f}\" for doc in relevant_documentation)\n",
    "    answer = f\"\"\"## Response\n",
    "### Question\n",
    "{question}\n",
    "### Answer\n",
    "{response}\n",
    "### Why?\n",
    "{likely_document['page_content']}\n",
    "* page: {likely_document['metadata']['page']}, relevance to answer: {likely_source[1]:.2f}\n",
    "* {likely_document['metadata']['source_document']}#page={likely_document['metadata']['page']}\n",
    "### Sources\n",
    "{sources}        \n",
    "    \"\"\"\n",
    "    IPython.display.display(IPython.display.Markdown(answer))\n",
    "        \n",
    "    #font = PIL.ImageFont.truetype(\"/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf\", 20)\n",
    "    document_image = page_images[likely_document['metadata']['page']-1]\n",
    "    vertices = documents[likely_document['metadata']['row']]['extras']['vertices']\n",
    "    draw = PIL.ImageDraw.Draw(document_image).polygon([\n",
    "        vertices[0]['x'], vertices[0]['y'],\n",
    "        vertices[1]['x'], vertices[1]['y'],\n",
    "        vertices[2]['x'], vertices[2]['y'],\n",
    "        vertices[3]['x'], vertices[3]['y']\n",
    "    ], outline = 'green', width = 5)\n",
    "    IPython.display.display(document_image.resize((800, 1000)))\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 261,
     "status": "ok",
     "timestamp": 1683730743606,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "X0ij5NJS8wvy",
    "outputId": "296af6b2-11ca-4be5-e4f6-1b9791ad8a57"
   },
   "outputs": [],
   "source": [
    "question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 3934,
     "status": "ok",
     "timestamp": 1683730715812,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "yuFyJoVjVJ1A",
    "outputId": "aa875c41-5782-475f-fb8d-7a48edfd57f0"
   },
   "outputs": [],
   "source": [
    "document_bot(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2164,
     "status": "ok",
     "timestamp": 1683730808508,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "EsHEO-szSxxS",
    "outputId": "a4ecb9ea-33f2-4bda-8a58-32333d948198"
   },
   "outputs": [],
   "source": [
    "document_bot(\"What is the definition of a balk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4565,
     "status": "ok",
     "timestamp": 1683730909879,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "DK9J55wJSxsg",
    "outputId": "f4fa9fd8-0656-411d-b51e-8ab5f5fff8aa"
   },
   "outputs": [],
   "source": [
    "document_bot(\"Is a rule broken if three infielders are positioned on the same side of the field where the batter is more likely to hit the ball?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 2990,
     "status": "ok",
     "timestamp": 1683731403294,
     "user": {
      "displayName": "Mike Henderson",
      "userId": "07691629187611687318"
     },
     "user_tz": 240
    },
    "id": "vNmawtEwJwr3",
    "outputId": "29be2d9d-079c-44b9-ff6b-f9af22dea3ab"
   },
   "outputs": [],
   "source": [
    "document_bot(\"A batter hits a ball that goes over the fence, but it is caught by a fan in the stands. Is the ball a home run?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFBf_d_s9TOE"
   },
   "outputs": [],
   "source": [
    "document_bot(\"What is the official size of a base?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zS_vOLFmNtQ2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOtJpcznd4BcDZJkSAIRVaj",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cpu.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m108"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
